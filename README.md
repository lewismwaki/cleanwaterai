# CleanWater AI ğŸ’§

## Project Overview

CleanWater AI is an end-to-end machine learning system for monitoring and predicting water quality in Kenya. The system integrates multiple data sources, applies machine learning models, and provides real-time insights through an interactive dashboard.

### Business Understanding
**Problem**: Access to clean water remains a critical challenge in Kenya, with limited real-time monitoring and predictive capabilities for water quality assessment.

**Solution**: An AI-powered system that:
- Monitors water point status and quality indicators
- Predicts contamination risks using environmental factors
- Provides real-time alerts and interactive visualizations
- Enables citizen reporting and community engagement

### Data Sources & Integration

**Primary Data Sources:**
1. **WPDx (Water Point Data Exchange)**: 22,000+ water points across Kenya with location, status, and infrastructure data
2. **GEMS (Global Environment Monitoring System)**: Water quality measurements including mercury and zinc contamination levels
3. **Google Earth Engine**: Environmental satellite data including NDVI, rainfall, temperature, soil moisture
4. **User Input**: Citizen reports on water quality observations (color, clarity, odor, infrastructure)

**Output Formats:**
- **Interactive Dashboard**: Streamlit web application with maps, charts, and alerts
- **Real-time Predictions**: ML model predictions on water quality risks
- **Alert System**: Automated notifications for contamination risks
- **CSV Exports**: Downloadable datasets for further analysis

## Project Architecture & Data Flow

```
Data Extraction â†’ Data Processing â†’ Model Training â†’ Deployment â†’ Monitoring
     â†“                â†“               â†“            â†“           â†“
   WPDx API      â†’  Cleaning     â†’  XGBoost    â†’  Streamlit  â†’  Alerts
   Gems/GE       â†’  Merging      â†’  NLP Model  â†’  Live API   â†’  Reports
   User Input    â†’  Feature Eng  â†’  Evaluation â†’  Dashboard  â†’  Feedback
```

### Folder Structure

```
cleanwaterai/
â”œâ”€â”€ ğŸ“ app/                                       # Streamlit application & live services
â”‚   â”œâ”€â”€ streamlit_app.py                          # Main dashboard interface
â”‚   â”œâ”€â”€ trigger_ingestion.py                      # Live data ingestion scheduler
â”‚   â”œâ”€â”€ trigger_predictions.py                    # Real-time prediction engine
â”‚   â”œâ”€â”€ trigger_alerts.py                         # Alert notification system
â”‚   â”œâ”€â”€ trigger_reports.py                        # Minimal queried CSVs for specific purposes
â”‚
â”œâ”€â”€ ğŸ“ data/                                      # Data storage hierarchy
â”‚   â”œâ”€â”€ raw/                                      # Original source data
â”‚   â”‚   â”œâ”€â”€ wpdx_kenya.csv                        # Water Point Data Exchange (22K points)
â”‚   â”‚   â”œâ”€â”€ ndvi_scaled.csv                       # Satellite environmental data
â”‚   â”‚   â”œâ”€â”€ mercury.csv                           # GEMS mercury contamination data
â”‚   â”‚   â””â”€â”€ zinc.csv                              # GEMS zinc contamination data
â”‚   â””â”€â”€ processed/                                # Cleaned, merged datasets
â”‚       â”œâ”€â”€ environmental.csv                     # Processed environmental features
â”‚       â”œâ”€â”€ gems.csv                              # Processed GEMS water quality data
â”‚       â””â”€â”€ nlp.csv                               # Processed text analysis data
â”‚
â”œâ”€â”€ ğŸ“ scripts/                                   # Core data pipeline scripts
â”‚   â”œâ”€â”€ extract_data.py                           # WPDx API data extraction
â”‚   â”œâ”€â”€ merge_data.py                             # Data integration & joining
â”‚   â”œâ”€â”€ prepare_data.py                           # Feature engineering pipeline
â”‚   â”œâ”€â”€ train_xgb.py                              # XGBoost model training
â”‚   â”œâ”€â”€ train_nlp.py                              # NLP model for text analysis
â”‚   â”œâ”€â”€ ingest_live_wpdx.py                       # Live WPDx updates
â”‚   â”œâ”€â”€ ingest_live_gee.py                        # Live satellite data
â”‚   â”œâ”€â”€ ingest_live_nlp.py                        # Live text processing
â”‚   â””â”€â”€ deploy.py                                 # Model deployment utilities
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                                 # Jupyter analysis notebooks (CRISP-DM flow)
â”‚   â”œâ”€â”€ 01_data_extraction.ipynb                  # Data Understanding
â”‚   â”œâ”€â”€ 02_data_cleaning.ipynb                    # Data Preparation
â”‚   â”œâ”€â”€ 03_data_merging.ipynb                     # Data Integration
â”‚   â”œâ”€â”€ 04_data_preparation.ipynb                 # Feature Engineering
â”‚   â”œâ”€â”€ 05_xgboost_model_training.ipynb           # Modeling
â”‚   â”œâ”€â”€ 06_nlp_model_training.ipynb               # Text Analytics
â”‚   â”œâ”€â”€ 07_model_evaluation.ipynb                 # Model Assessment
â”‚   â”œâ”€â”€ 08_model_interpretability.ipynb           # Model Insights
â”‚   â”œâ”€â”€ 09_deployment.ipynb                       # Deployment Strategy
â”‚   â””â”€â”€ 10_monitoring_and_maintenance.ipynb       # Production Monitoring
â”‚
â”œâ”€â”€ ğŸ“ models/                                   # Trained model artifacts
â”‚   â””â”€â”€ nlp.pkl                                   # NLP model for text classification
â”‚
â”œâ”€â”€ ğŸ“ reports/                                  # Documentation & presentations
â”‚   â”œâ”€â”€ prd.pdf                                   # Product Requirements Document
â”‚   â”œâ”€â”€ presentation.pdf                          # Project presentation
â”‚   â””â”€â”€ report.pdf                                # Technical report
â”‚
â”œâ”€â”€ main.py                                       # Main orchestration starting point
â”œâ”€â”€ requirements.txt                              # Python dependencies
â”œâ”€â”€ pyproject.toml                                # Build configuration
â””â”€â”€ setup.py                                      # Package installation
```

### Data Flow Architecture

**1. Data Ingestion Pipeline:**
```
WPDx API â†’ extract_data.py â†’ data/raw/wpdx_kenya.csv
GEMS â†’ mercury/zinc processing â†’ data/raw/mercury.csv, data/raw/zinc.csv
GEE API â†’ satellite processing â†’ data/raw/ndvi_scaled.csv
User Input â†’ streamlit_app.py â†’ real-time processing
```

**2. Data Processing Pipeline:**
```
Raw Data â†’ merge_data.py â†’ Data Integration
Integrated Data â†’ prepare_data.py â†’ Feature Engineering
Features â†’ data/processed/final_wpdx_environmental_data.csv
```

**3. Model Training Pipeline:**
```
Processed Data â†’ train_xgb.py â†’ XGBoost Model (water quality prediction)
Text Data â†’ train_nlp.py â†’ NLP Model (text classification)
Models â†’ Evaluation â†’ models/ directory
```

**4. Live System Pipeline:**
```
trigger_ingestion.py â†’ Live data updates every hour
trigger_predictions.py â†’ Real-time ML predictions
trigger_alerts.py â†’ Automated risk notifications
streamlit_app.py â†’ Interactive dashboard display
```

## Quick Start Guide

### Prerequisites
- **Python Version**: 3.8+ (tested on 3.12)
- **Operating System**: Windows, macOS, or Linux
- **Memory**: Minimum 4GB RAM recommended

### Installation & Setup

1. **Clone and Navigate to Project:**
```bash
git clone https://github.com/TonnieD/CleanWatAI.git
cd cleanwaterai
```

2. **Run Setup (creates conda environment and installs everything):**
```bash
python setup.py
```

3. **Activate Environment:**
```bash
conda activate cleanwaterai_env
```

### Running the Complete Pipeline

**Option A: Full Automated Pipeline**
```bash
python main.py
```
This will:
- Execute all notebooks sequentially (01-10)
- Run all data scripts (extract â†’ merge â†’ prepare â†’ train)
- Start live ingestion, predictions, and alerts
- Launch Streamlit dashboard on port 8505

**Option B: Manual Step-by-Step Execution**

1. **Data Extraction:**
```bash
python scripts/extract_data.py
```

2. **Data Processing:**
```bash
python scripts/merge_data.py
python scripts/prepare_data.py
```

3. **Model Training:**
```bash
python scripts/train_xgb.py
python scripts/train_nlp.py
```

4. **Start Live Services:**
```bash
python app/trigger_ingestion.py
python app/trigger_predictions.py
python app/trigger_alerts.py
```

5. **Launch Dashboard:**
```bash
streamlit run app/streamlit_app.py --server.port 8505
```

### Working with Notebooks

**Sequential Analysis (CRISP-DM methodology):**
```bash
jupyter notebook notebooks/01_data_extraction.ipynb
# Continue through 02-10 in order
```

Each notebook builds on the previous one:
- **01-04**: Data understanding and preparation
- **05-06**: Model development and training  
- **07-08**: Model evaluation and interpretation
- **09-10**: Deployment and monitoring strategies

### Dashboard Features

The Streamlit application provides:

**Interactive Components:**
- ğŸ—ºï¸ **Risk Map**: Geospatial water quality visualization
- ğŸ“Š **Analytics Dashboard**: Real-time metrics and trends
- ğŸš¨ **Alert System**: Live contamination warnings
- ğŸ“ **Citizen Reporting**: Community water quality input
- ğŸ“ˆ **Trend Analysis**: Historical data visualization

**Real-time Data:**
- Live water point status updates
- Environmental satellite data integration
- ML-powered risk predictions
- Automated alert generation

### Development Workflow

**For New Contributors:**

1. **Environment Setup**: Follow installation steps above
2. **Data Pipeline**: Run `python main.py` to ensure full pipeline works
3. **Notebook Analysis**: Review notebooks 01-10 to understand methodology
4. **Feature Development**: Modify individual scripts in `scripts/` directory
5. **Dashboard Updates**: Edit `app/streamlit_app.py` for UI changes
6. **Testing**: Verify changes don't break the main pipeline

**Key Integration Points:**
- `scripts/extract_data.py` â†” External APIs (WPDx, Google Earth Engine)
- `data/processed/` â†” ML models for training and prediction
- `app/trigger_*.py` â†” Live data ingestion and alert systems
- `app/streamlit_app.py` â†” User interface and visualization

### Configuration Notes

**Environment Variables** (if needed):
- Google Earth Engine authentication
- Google CloudAPI keys for external services
- Database connection strings (for production)

**Performance Considerations:**
- Initial data extraction may take 15-20 minutes (22K records)
- Model training requires ~2GB memory for full dataset
- Dashboard loads data from CSV files (consider database for production)

### Troubleshooting

**Common Issues:**
1. **Memory errors**: Reduce batch size in data processing scripts
2. **API timeouts**: Check internet connection and API rate limits
3. **Missing models**: Ensure `scripts/train_*.py` completed successfully
4. **Dashboard errors**: Verify all CSV files exist in `data/` directories

**Support**: Check individual script documentation and notebook markdown cells for detailed implementation notes.
