# CleanWater AI üíß

## Project Overview

CleanWater AI is an end-to-end machine learning system for monitoring and predicting water quality in Kenya. The system integrates multiple data sources, applies machine learning models, and provides real-time insights through an interactive dashboard.

### Business Understanding
**Problem**: Access to clean water remains a critical challenge in Kenya, with limited real-time monitoring and predictive capabilities for water quality assessment.

**Solution**: An AI-powered system that:
- Monitors water point status and quality indicators
- Predicts contamination risks using environmental factors
- Provides real-time alerts and interactive visualizations
- Enables citizen reporting and community engagement

### Data Sources & Integration

**Primary Data Sources:**
1. **WPDx (Water Point Data Exchange)**: 22,000+ water points across Kenya with location, status, and infrastructure data
2. **Google Earth Engine**: Environmental satellite data including NDVI, rainfall, temperature, soil moisture
3. **User Input**: Citizen reports on water quality observations (color, clarity, odor, infrastructure)

**Output Formats:**
- **Interactive Dashboard**: Streamlit web application with maps, charts, and alerts
- **Real-time Predictions**: ML model predictions on water quality risks
- **Alert System**: Automated notifications for contamination risks
- **CSV Exports**: Downloadable datasets for further analysis

## Project Architecture & Data Flow

```
Data Extraction ‚Üí Data Processing ‚Üí Model Training ‚Üí Deployment ‚Üí Monitoring
     ‚Üì                ‚Üì               ‚Üì            ‚Üì           ‚Üì
   WPDx API      ‚Üí  Cleaning     ‚Üí  XGBoost    ‚Üí  Streamlit  ‚Üí  Alerts
   GEE API       ‚Üí  Merging      ‚Üí  NLP Model  ‚Üí  Live API   ‚Üí  Updates
   User Input    ‚Üí  Feature Eng  ‚Üí  Evaluation ‚Üí  Dashboard  ‚Üí  Feedback
```

### Folder Structure

```
cleanwaterai/
‚îú‚îÄ‚îÄ üìÅ app/                                       # Streamlit application & live services
‚îÇ   ‚îú‚îÄ‚îÄ streamlit_app.py                          # Main dashboard interface
‚îÇ   ‚îú‚îÄ‚îÄ trigger_ingestion.py                      # Live data ingestion scheduler
‚îÇ   ‚îú‚îÄ‚îÄ trigger_predictions.py                    # Real-time prediction engine
‚îÇ   ‚îú‚îÄ‚îÄ trigger_alerts.py                         # Alert notification system
‚îÇ   ‚îî‚îÄ‚îÄ index.css                                 # Dashboard styling
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                                      # Data storage hierarchy
‚îÇ   ‚îú‚îÄ‚îÄ raw/                                      # Original source data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wpdx_kenya.csv                        # Water Point Data Exchange (22K points)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ndvi_scaled.csv                       # Satellite environmental data
‚îÇ   ‚îî‚îÄ‚îÄ processed/                                # Cleaned, merged datasets
‚îÇ       ‚îî‚îÄ‚îÄ final_wpdx_environmental_data.csv     # ML-ready dataset
‚îÇ
‚îú‚îÄ‚îÄ üìÅ scripts/                                   # Core data pipeline scripts
‚îÇ   ‚îú‚îÄ‚îÄ extract_data.py                           # WPDx API data extraction
‚îÇ   ‚îú‚îÄ‚îÄ merge_data.py                             # Data integration & joining
‚îÇ   ‚îú‚îÄ‚îÄ prepare_data.py                           # Feature engineering pipeline
‚îÇ   ‚îú‚îÄ‚îÄ train_xgb.py                              # XGBoost model training
‚îÇ   ‚îú‚îÄ‚îÄ train_nlp.py                              # NLP model for text analysis
‚îÇ   ‚îú‚îÄ‚îÄ ingest_live_wpdx.py                       # Live WPDx updates
‚îÇ   ‚îú‚îÄ‚îÄ ingest_live_gee.py                        # Live satellite data
‚îÇ   ‚îú‚îÄ‚îÄ ingest_live_nlp.py                        # Live text processing
‚îÇ   ‚îî‚îÄ‚îÄ deploy.py                                 # Model deployment utilities
‚îÇ
‚îú‚îÄ‚îÄ üìÅ notebooks/                                 # Jupyter analysis notebooks (CRISP-DM flow)
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_extraction.ipynb                  # Data Understanding
‚îÇ   ‚îú‚îÄ‚îÄ 02_data_cleaning.ipynb                    # Data Preparation
‚îÇ   ‚îú‚îÄ‚îÄ 03_data_merging.ipynb                     # Data Integration
‚îÇ   ‚îú‚îÄ‚îÄ 04_data_preparation.ipynb                 # Feature Engineering
‚îÇ   ‚îú‚îÄ‚îÄ 05_xgboost_model_training.ipynb           # Modeling
‚îÇ   ‚îú‚îÄ‚îÄ 06_nlp_model_training.ipynb               # Text Analytics
‚îÇ   ‚îú‚îÄ‚îÄ 07_model_evaluation.ipynb                 # Model Assessment
‚îÇ   ‚îú‚îÄ‚îÄ 08_model_interpretability.ipynb           # Model Insights
‚îÇ   ‚îú‚îÄ‚îÄ 09_deployment.ipynb                       # Deployment Strategy
‚îÇ   ‚îî‚îÄ‚îÄ 10_monitoring_and_maintenance.ipynb       # Production Monitoring
‚îÇ
‚îú‚îÄ‚îÄ üìÅ models/                                   # Trained model artifacts
‚îÇ   ‚îî‚îÄ‚îÄ nlp.pkl                                   # NLP model for text classification
‚îÇ
‚îú‚îÄ‚îÄ üìÅ reports/                                  # Documentation & presentations
‚îÇ   ‚îú‚îÄ‚îÄ prd.pdf                                   # Product Requirements Document
‚îÇ   ‚îú‚îÄ‚îÄ presentation.pdf                          # Project presentation
‚îÇ   ‚îî‚îÄ‚îÄ report.pdf                                # Technical report
‚îÇ
‚îú‚îÄ‚îÄ main.py                                       # Main orchestration starting point
‚îú‚îÄ‚îÄ requirements.txt                              # Python dependencies
‚îú‚îÄ‚îÄ pyproject.toml                                # Build configuration
‚îî‚îÄ‚îÄ setup.py                                      # Package installation
```

### Data Flow Architecture

**1. Data Ingestion Pipeline:**
```
WPDx API ‚Üí extract_data.py ‚Üí data/raw/wpdx_kenya.csv
GEE API ‚Üí satellite processing ‚Üí data/raw/ndvi_scaled.csv
User Input ‚Üí streamlit_app.py ‚Üí real-time processing
```

**2. Data Processing Pipeline:**
```
Raw Data ‚Üí merge_data.py ‚Üí Data Integration
Integrated Data ‚Üí prepare_data.py ‚Üí Feature Engineering
Features ‚Üí data/processed/final_wpdx_environmental_data.csv
```

**3. Model Training Pipeline:**
```
Processed Data ‚Üí train_xgb.py ‚Üí XGBoost Model (water quality prediction)
Text Data ‚Üí train_nlp.py ‚Üí NLP Model (text classification)
Models ‚Üí Evaluation ‚Üí models/ directory
```

**4. Live System Pipeline:**
```
trigger_ingestion.py ‚Üí Live data updates every hour
trigger_predictions.py ‚Üí Real-time ML predictions
trigger_alerts.py ‚Üí Automated risk notifications
streamlit_app.py ‚Üí Interactive dashboard display
```

## Quick Start Guide

### Prerequisites
- **Python Version**: 3.8+ (tested on 3.12)
- **Operating System**: Windows, macOS, or Linux
- **Memory**: Minimum 4GB RAM recommended

### Installation & Setup

1. **Clone and Navigate to Project:**
```bash
git clone https://github.com/TonnieD/CleanWatAI.git
cd cleanwatai
```

2. Create a pyproject.toml file with the following content:
```toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"
```

3. **Install Package in Development Mode:**
```bash
pip install -e .
```

4. **Install Dependencies:**
```bash
pip install -r requirements.txt
```

### Running the Complete Pipeline

**Option A: Full Automated Pipeline**
```bash
python main.py
```
This will:
- Execute all notebooks sequentially (01-10)
- Run all data scripts (extract ‚Üí merge ‚Üí prepare ‚Üí train)
- Start live ingestion, predictions, and alerts
- Launch Streamlit dashboard on port 8505

**Option B: Manual Step-by-Step Execution**

1. **Data Extraction:**
```bash
python scripts/extract_data.py
```

2. **Data Processing:**
```bash
python scripts/merge_data.py
python scripts/prepare_data.py
```

3. **Model Training:**
```bash
python scripts/train_xgb.py
python scripts/train_nlp.py
```

4. **Start Live Services:**
```bash
python app/trigger_ingestion.py
python app/trigger_predictions.py
python app/trigger_alerts.py
```

5. **Launch Dashboard:**
```bash
streamlit run app/streamlit_app.py --server.port 8505
```

### Working with Notebooks

**Sequential Analysis (CRISP-DM methodology):**
```bash
jupyter notebook notebooks/01_data_extraction.ipynb
# Continue through 02-10 in order
```

Each notebook builds on the previous one:
- **01-04**: Data understanding and preparation
- **05-06**: Model development and training  
- **07-08**: Model evaluation and interpretation
- **09-10**: Deployment and monitoring strategies

### Dashboard Features

The Streamlit application provides:

**Interactive Components:**
- üó∫Ô∏è **Risk Map**: Geospatial water quality visualization
- üìä **Analytics Dashboard**: Real-time metrics and trends
- üö® **Alert System**: Live contamination warnings
- üìù **Citizen Reporting**: Community water quality input
- üìà **Trend Analysis**: Historical data visualization

**Real-time Data:**
- Live water point status updates
- Environmental satellite data integration
- ML-powered risk predictions
- Automated alert generation

### Development Workflow

**For New Contributors:**

1. **Environment Setup**: Follow installation steps above
2. **Data Pipeline**: Run `python main.py` to ensure full pipeline works
3. **Notebook Analysis**: Review notebooks 01-10 to understand methodology
4. **Feature Development**: Modify individual scripts in `scripts/` directory
5. **Dashboard Updates**: Edit `app/streamlit_app.py` for UI changes
6. **Testing**: Verify changes don't break the main pipeline

**Key Integration Points:**
- `scripts/extract_data.py` ‚Üî External APIs (WPDx, Google Earth Engine)
- `data/processed/` ‚Üî ML models for training and prediction
- `app/trigger_*.py` ‚Üî Live data ingestion and alert systems
- `app/streamlit_app.py` ‚Üî User interface and visualization

### Configuration Notes

**Environment Variables** (if needed):
- Google Earth Engine authentication
- Google CloudAPI keys for external services
- Database connection strings (for production)

**Performance Considerations:**
- Initial data extraction may take 15-20 minutes (22K records)
- Model training requires ~2GB memory for full dataset
- Dashboard loads data from CSV files (consider database for production)

### Troubleshooting

**Common Issues:**
1. **Memory errors**: Reduce batch size in data processing scripts
2. **API timeouts**: Check internet connection and API rate limits
3. **Missing models**: Ensure `scripts/train_*.py` completed successfully
4. **Dashboard errors**: Verify all CSV files exist in `data/` directories

**Support**: Check individual script documentation and notebook markdown cells for detailed implementation notes.
